{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3900b75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docx import Document as DocxDocument\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pickle, os\n",
    "from datetime import datetime\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.docstore.document import Document\n",
    "from datetime import datetime\n",
    "import re\n",
    "from pathlib import Path\n",
    "from __future__ import annotations\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import re, requests, html\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from urllib.parse import urljoin, urlparse\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650bbde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "chunks = []\n",
    "CHUNK_TOKENS   = 200\n",
    "CHUNK_OVERLAP  = 20\n",
    "SEPARATORS     = [\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size      = CHUNK_TOKENS * 6,   # ~400 tokens ≈ 2400 chars\n",
    "    chunk_overlap   = CHUNK_OVERLAP * 6,  # keep ~40‑token context\n",
    "    separators      = SEPARATORS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a31cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean(text: str) -> str:\n",
    "    \"\"\"Remove common TOC dots, extra whitespace, etc.\"\"\"\n",
    "    text = re.sub(r'\\.{5,}\\s*\\d+', '', text)   # dotted TOC lines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)     # huge blank gaps\n",
    "    return text.strip()\n",
    "\n",
    "def load_and_chunk_docx(path: str | Path) -> list[Document]:\n",
    "    docx = DocxDocument(path)\n",
    "    file_name = Path(path).name\n",
    "    uploaded  = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # 1️⃣ Collect paragraphs with their nearest heading\n",
    "    current_heading = \"Untitled\"\n",
    "    paragraphs      = []\n",
    "    for para in docx.paragraphs:\n",
    "        style = para.style.name\n",
    "        text  = para.text.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        if style.startswith(\"Heading\"):          # e.g., 'Heading 1'\n",
    "            current_heading = text\n",
    "            continue\n",
    "\n",
    "        paragraphs.append({\"heading\": current_heading, \"text\": text})\n",
    "\n",
    "    # 2️⃣ Group paragraphs by heading to reduce splitter fragmentation\n",
    "    grouped = {}\n",
    "    for p in paragraphs:\n",
    "        grouped.setdefault(p[\"heading\"], []).append(p[\"text\"])\n",
    "\n",
    "    docs = []\n",
    "    for heading, para_list in grouped.items():\n",
    "        combined = _clean(\"\\n\".join(para_list))\n",
    "        if not combined:\n",
    "            continue\n",
    "\n",
    "        # 3️⃣ Split into semantic chunks\n",
    "        chunks = splitter.split_text(combined)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            meta = {\n",
    "                \"source\": file_name,\n",
    "                \"section\": heading,\n",
    "                \"chunk_index\": idx,\n",
    "                \"uploaded_at\": uploaded,\n",
    "            }\n",
    "            docs.append(Document(page_content=chunk, metadata=meta))\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aee4ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_faiss_index():\n",
    "    if os.path.exists(\"faiss.idx\"):\n",
    "        print(\"FAISS index already exists, skipping creation.\")\n",
    "        return\n",
    "\n",
    "    print(\"Creating FAISS index…\")\n",
    "    chunks = []\n",
    "\n",
    "    # local .docx files\n",
    "    training = \"training\"\n",
    "    for fname in os.listdir(training):\n",
    "        if fname.startswith(\".\"):          # skip hidden files\n",
    "            continue\n",
    "        docs = load_and_chunk_docx(os.path.join(training, fname))\n",
    "        chunks.extend(splitter.split_documents(docs))\n",
    "\n",
    "    # # 2. live URLs\n",
    "    # for url in urls:\n",
    "    #     try:\n",
    "    #         docs = load_and_chunk_docx(url)           # << only the URL string here\n",
    "    #         chunks.extend(splitter.split_documents(docs))\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"[WARN] {url} skipped – {e}\")\n",
    "\n",
    "    print(f\"{len(chunks)} chunks total\")\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    db = FAISS.from_documents(chunks, embeddings)\n",
    "    db.save_local(\"faiss.idx\")\n",
    "    print(\"FAISS index created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f52d09a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index…\n",
      "79 chunks total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Ai\\ai-env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created and saved.\n"
     ]
    }
   ],
   "source": [
    "save_faiss_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b638d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b348feb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
